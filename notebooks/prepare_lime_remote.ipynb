{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME signature for remote model\n",
    "\n",
    "We have to adapt the LIME signature computation to work with the MLaaS model.\n",
    "We will generate first all the perturbed points that LIME needs to generate its local models.\n",
    "\n",
    "Then, we will transform all vectors to images, save them as jpg files and upload them to the MLaaS platform. \n",
    "With that done, we will be able to perform a batch classification operation and obtain the results for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "from zest import utils\n",
    "from zest import model\n",
    "from zest import lime_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], \n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "def show_torch_float(i):\n",
    "    a = transforms.functional.to_pil_image(i.to(torch.uint8))\n",
    "    plt.imshow(a)\n",
    "    plt.show()\n",
    "\n",
    "def show_torch_advex(img):\n",
    "    # img = img / 2 + 0.5     # unnormalize\n",
    "    img = inv_normalize(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CIFAR10'\n",
    "batch_size = 32\n",
    "dist = ['1', '2', 'inf', 'cos']\n",
    "lime_data_name = f\"{dataset}_{batch_size}_lime\"\n",
    "save_name = lime_data_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reproduce the behavior of the `lime()` function.\n",
    "\n",
    "```python\n",
    "# Original LIME function from Zest \n",
    "def lime(self, save_name=None, cat=True):\n",
    "    if save_name is None:\n",
    "        save_name = self.lime_data_name\n",
    "    self.net.eval()\n",
    "    if self.lime_data is None:\n",
    "        self.lime_data = lime_pytorch.prepare_lime_ref_data(save_name, self.trainset, self.batch_size)\n",
    "    if self.lime_segment is None:\n",
    "        self.lime_segment = lime_pytorch.prepare_lime_segment(save_name, self.lime_data, self.trainset)\n",
    "    if self.ref_dataset is None or self.lime_dataset is None:\n",
    "        self.ref_dataset, self.lime_dataset = lime_pytorch.prepare_lime_dataset(save_name, self.lime_data,\n",
    "                                                                                self.lime_segment)\n",
    "    self.lime_mask = lime_pytorch.compute_lime_signature(self.net, self.ref_dataset, self.lime_dataset, cat=cat)\n",
    "    self.net.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = utils.load_dataset(dataset, True, download=True)\n",
    "lime_data = lime_pytorch.prepare_lime_ref_data(save_name, trainset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_segment = lime_pytorch.prepare_lime_segment(save_name, lime_data, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32, 3)\n",
      "(32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(lime_data.shape)\n",
    "print(lime_segment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dataset, lime_dataset = lime_pytorch.prepare_lime_dataset(save_name, lime_data, lime_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1000, 32, 32, 3)\n",
      "(32,)\n",
      "(1000, 23)\n"
     ]
    }
   ],
   "source": [
    "print(ref_dataset.shape)\n",
    "print(lime_dataset.shape)\n",
    "print(lime_dataset[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't call `compute_lime_signature()` directly because it internally calls `label_lime_dataset()` which requires a model to be loaded. So we will adapt it to the remote model.\n",
    "\n",
    "\n",
    "```python\n",
    "# Original label data function from Zest\n",
    "def label_lime_dataset(lime_dataset, ref_dataset, model):\n",
    "    device = torch.device('cuda:0' if next(model.parameters()).is_cuda else 'cpu')\n",
    "    datasets = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(lime_dataset)):\n",
    "            lime_data = lime_dataset[i]\n",
    "            data = ref_dataset[i]\n",
    "            inputs = torch.from_numpy(data).to(device).permute(0, 3, 1, 2).float()\n",
    "            outputs = model(inputs).detach().cpu().numpy()\n",
    "            datasets.append([lime_data, outputs])\n",
    "    return datasets\n",
    "```\n",
    "\n",
    "We need the model outputs for each sample in the `ref_dataset` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/data/fedpois/lime32\n",
      "gs://bad-lemon-vcm/lime32/\n",
      "data/lime32.jsonl\n"
     ]
    }
   ],
   "source": [
    "base_save_path = '/net/data/fedpois/lime{}'.format(batch_size)\n",
    "os.makedirs(base_save_path, exist_ok=True)\n",
    "jsonl_file = 'data/lime{}.jsonl'.format(batch_size)\n",
    "remote_base_pth = 'gs://bad-lemon-vcm/lime{}/'.format(batch_size)\n",
    "print(base_save_path)\n",
    "print(remote_base_pth)\n",
    "print(jsonl_file)\n",
    "\n",
    "# Example\n",
    "# {\"content\": \"gs://sourcebucket/datasets/images/source_image.jpg\", \"mimeType\": \"image/jpeg\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Waning**: This creates a **lot** of jpg files. Make sure you have setup the paths correctly, and only execute this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9cb8536b67f48a68e6df9dfbaebd9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(jsonl_file, 'w') as mf:    \n",
    "\n",
    "    for base_img in tqdm(range(ref_dataset.shape[0])):\n",
    "        cur_base = torch.from_numpy(ref_dataset)[base_img].permute(0, 3, 1, 2)\n",
    "        # print('Shape of the current base image tensor:', cur_base.shape)\n",
    "\n",
    "        for pert_idx, pert_img in enumerate(cur_base):\n",
    "            unnorm = inv_normalize(pert_img).float()\n",
    "            # print('Shape of the {}-th perturbation image tensor: {}'.format(pert_idx, unnorm.shape))\n",
    "\n",
    "            unnorm_pil = transforms.functional.to_pil_image(unnorm)\n",
    "\n",
    "            save_pth = os.path.join(base_save_path, f'{base_img}_{pert_idx}.jpg')\n",
    "            unnorm_pil.save(save_pth)\n",
    "\n",
    "            remote_name = remote_base_pth + '{}_{}.jpg'.format(base_img, pert_idx)\n",
    "\n",
    "            line_base = {\"content\": remote_name, \"mimeType\": \"image/jpeg\"}\n",
    "            mf.write(json.dumps(line_base) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow( transforms.functional.to_pil_image( inv_normalize( torch.from_numpy(ref_dataset)[0].permute(0, 3, 1, 2)[0] ).float() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms.functional.pil_to_tensor( transforms.functional.to_pil_image( inv_normalize( torch.from_numpy(ref_dataset)[0].permute(0, 3, 1, 2)[0] ).float() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dedc6e9d36f5fe0ab4d9e7a27bf278c6ab614087e8708f25ce911079ab243912"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lemon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
